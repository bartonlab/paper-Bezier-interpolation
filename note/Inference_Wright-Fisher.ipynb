{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "eeca9beb",
   "metadata": {},
   "outputs": [],
   "source": [
    "using DelimitedFiles\n",
    "using StatsBase \n",
    "using Random\n",
    "using LinearAlgebra\n",
    "using Plots\n",
    "using LaTeXStrings\n",
    "using Printf\n",
    "rng = Random.MersenneTwister(1234);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a4e93f17",
   "metadata": {},
   "outputs": [],
   "source": [
    "include(\"../src/WF/MPL_Tools.jl\")\n",
    "include(\"../src/WF/MPL_run_binary.jl\")\n",
    "include(\"../src/WF/Bezier_interpolation_binary_filter.jl\")\n",
    "include(\"../src/WF/functions_performance_check.jl\");\n",
    "include(\"../src/common/common_Bezier.jl\");\n",
    "include(\"../src/common/common_analysis.jl\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57dac093",
   "metadata": {},
   "source": [
    "# Make trajectory file for the Wright-Fisher model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "10ead9f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_key1 =  \"allele-traject_id-\"\n",
    "data_dir = \"../data/WF/\"\n",
    "file_key2 = \"_mu-0.001_N-1000_WF_standard.txt\"\n",
    "dir_out = \"../out/WF/\"\n",
    "\n",
    "\n",
    "L = 50\n",
    "N = 1000;\n",
    "time_upto = 300;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f99eb234",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell should be enclosed. \n",
    "for id_ensemble in 1:10\n",
    "    (data, time_list) = get_data_time(file_key1, file_key2, data_dir, id_ensemble, time_upto)\n",
    "    time_list = unique(time_list) \n",
    "    n_time = length(time_list)\n",
    "    mean_x1 = zeros(n_time,L)\n",
    "    mean_x1 = zeros(n_time, L)\n",
    "\n",
    "    for id_t in 1:n_time\n",
    "        t = time_list[id_t]\n",
    "        (n_t, sample_t) = get_sample_at_t(data, t); # without interpolate\n",
    "        (x1_t1, x2_t1) = get_x1_x2(L, n_t, sample_t)\n",
    "        mean_x1[id_t, :] = copy(x1_t1)\n",
    "    end    \n",
    "    fout = open(dir_out * \"WF_trajectories/WF-trajectory_id-\"*string(id_ensemble)*\".txt\", \"w\");\n",
    "    for id_t in 1:n_time\n",
    "        for i in 1:L \n",
    "            print(fout, @sprintf(\"%.5f \", mean_x1[id_t, i]) )\n",
    "        end\n",
    "        println(fout, \"\")\n",
    "    end\n",
    "    close(fout);\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73c5e3be",
   "metadata": {},
   "source": [
    "# Infer seleciton coefficients for sequences generated from Wright-Fisher models "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "46c68f61",
   "metadata": {},
   "outputs": [],
   "source": [
    "mu = 1e-3\n",
    "time_steps = [1, 10, 30, 60, 75, 80, 100, 95]\n",
    "gamma_set = [0, 1e-3, 0.1, 0.5, 1, 2, 5, 10, 30]\n",
    "time_cutoff = 300\n",
    "n_ensemble = 100 \n",
    "N_sub_population = 750;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e9aabba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell compute selection coefficients for 1000 enesembles and estimate probability denstiy of selection. \n",
    "# The total computation tims is 1300 sec.\n",
    "n_time_steps = size(time_steps,1)\n",
    "n_gamma_max = size(gamma_set,1)\n",
    "selections_B = zeros(n_time_steps, n_ensemble, n_gamma_max, L)\n",
    "selections_L = zeros(n_time_steps, n_ensemble, n_gamma_max, L)\n",
    "selections_naive = zeros(n_time_steps, n_ensemble, n_gamma_max, L)\n",
    "\n",
    "selections_B_SL = zeros(n_time_steps, n_ensemble, n_gamma_max, L)\n",
    "selections_L_SL = zeros(n_time_steps, n_ensemble, n_gamma_max, L)\n",
    "selections_naive_SL = zeros(n_time_steps, n_ensemble, n_gamma_max, L)\n",
    "\n",
    "eigen_min_B = zeros(n_time_steps, n_ensemble)\n",
    "eigen_min_L = zeros(n_time_steps, n_ensemble)\n",
    "eigen_min_naive = zeros(n_time_steps, n_ensemble)\n",
    "\n",
    "errorbar_Bez = zeros(n_time_steps, n_ensemble, L)\n",
    "errorbar_Lin = zeros(n_time_steps, n_ensemble, L)\n",
    "\n",
    "#----- enhance genetic drift. Turning on this function takes a long computation ---- #\n",
    "### Bezier has advantage when large genetic drift presens, a small sub population size.\n",
    "flag_large_genetic_drift = true\n",
    "#-----------------------------\n",
    "\n",
    "Population_tot = copy(N)\n",
    "\n",
    "for n in 1:n_ensemble     \n",
    "    for id_time_steps in 1:n_time_steps\n",
    "        #----------- obtaining data ---------------#\n",
    "        t_sampling = time_steps[id_time_steps]\n",
    "        #fname_in = \"../data/WF/allele-traject-time-1_id-\" *string(n)* \"_mu-0.001_N-1000_standard.txt\"\n",
    "        #fname_in = \"../data/WF/allele-traject_id-\" *string(n)* \"_mu-0.001_N-1000_WF_standard.txt\"\n",
    "        fname_in = data_dir * file_key1 *string(n)* file_key2\n",
    "        # this is test\n",
    "        #fname_in = \"../data_temp/WF/N100/allele-traject_id-\" *string(n)* \"_mu-0.001_N-100_WF_standard.txt\"\n",
    "        \n",
    "        data = readdlm(fname_in);\n",
    "        read_upto = count(data[:,1] .<= time_cutoff)\n",
    "        data = copy(data[1:read_upto, :])\n",
    "        \n",
    "        ## selecting samples those collecting times are each t steps, 0, 30, 60, etc.\n",
    "        selecting_indices = collect(1:size(data,1))[data[:,1] .% t_sampling .== 0]\n",
    "        data = data[selecting_indices,:];\n",
    "        \n",
    "        ## resumpling from the large population\n",
    "        if(flag_large_genetic_drift)\n",
    "            data = resumpling_data(data, N, N_sub_population)\n",
    "        end\n",
    "        \n",
    "        time_list = get_time_list(data);\n",
    "        L = size(data,2)-3\n",
    "        unique_time = unique(time_list);\n",
    "        \n",
    "        #------------ obtaining covariances and drifts -----------#\n",
    "        #(C_tot_B, drift_B, x1_mean_B, x1_init_B, x1_fini_B, point_set_x1_a_B, point_set_x1_b_B, point_set_x2_a_B, point_set_x2_b_B) = get_Bezier_Ctot_drift_x1mean_binary_simple(L, data, unique_time);\n",
    "        time_interval_threshold = 90\n",
    "        (C_tot_B, drift_B, x1_mean_B, x1_init_B, x1_fini_B, point_set_x1_a_B, point_set_x1_b_B, point_set_x2_a_B, point_set_x2_b_B) = get_Bezier_Ctot_drift_x1mean_binary_simple_with_midpoints(L, data, unique_time, time_interval_threshold);\n",
    "        \n",
    "        (C_tot_L, drift_L, x1_mean_L, x1_init_L, x1_fini_L) = get_integrated_Ctot_drift_x1mean_binary_simple(L, data, unique_time);\n",
    "        (C_tot_naive, drift_naive, x1_mean_naive, x1_init_naive, x1_fini_naive) = get_naive_Ctot_drift_x1mean(L, data, unique_time)\n",
    "        \n",
    "        errorbar_Bez[id_time_steps, n, :] = copy(C_tot_B[diagind(C_tot_B)])\n",
    "        errorbar_Lin[id_time_steps, n, :] = copy(C_tot_L[diagind(C_tot_L)])\n",
    "        \n",
    "        eigen_min_B[id_time_steps, n] = minimum(eigen(C_tot_B).values)\n",
    "        eigen_min_L[id_time_steps, n] = minimum(eigen(C_tot_L).values)\n",
    "        eigen_min_naive[id_time_steps, n] = minimum(eigen(C_tot_naive).values)\n",
    "        \n",
    "        for n_gamma in 1:n_gamma_max\n",
    "            gamma = gamma_set[n_gamma]\n",
    "            \n",
    "            #----------- obtaining the seelctions. ---------------------#            \n",
    "            selections_B_temp = selection_coeff(mu, drift_B, C_tot_B + gamma*I, x1_init_B, x1_fini_B)\n",
    "            selections_L_temp = selection_coeff(mu, drift_L, C_tot_L + gamma*I, x1_init_L, x1_fini_L);\n",
    "            selections_naive_temp = selection_coeff(mu, drift_naive, C_tot_naive + gamma*I, x1_init_naive, x1_fini_naive);\n",
    "            #----------- register the estimated selections  ------------#\n",
    "            selections_B[id_time_steps, n, n_gamma, :] = copy(selections_B_temp)\n",
    "            selections_L[id_time_steps, n, n_gamma, :] = copy(selections_L_temp)\n",
    "            selections_naive[id_time_steps, n, n_gamma, :] = copy(selections_naive_temp)\n",
    "            #----------- for single locus (SL) methods -----------------#\n",
    "            \n",
    "            C_tot_B_SL = copy(diagm(0=>C_tot_B[diagind(C_tot_B)]))\n",
    "            C_tot_L_SL = copy(diagm(0=>C_tot_L[diagind(C_tot_L)]))\n",
    "            C_tot_naive_SL = copy(diagm(0=>C_tot_naive[diagind(C_tot_naive)]))\n",
    "            #----------- obtaining the seelctions. ---------------------#            \n",
    "            selections_B_temp_SL = selection_coeff(mu, drift_B, C_tot_B_SL + gamma*I, x1_init_B, x1_fini_B)\n",
    "            selections_L_temp_SL = selection_coeff(mu, drift_L, C_tot_L_SL + gamma*I, x1_init_L, x1_fini_L);\n",
    "            selections_naive_temp_SL = selection_coeff(mu, drift_naive, C_tot_naive_SL + gamma*I, x1_init_naive, x1_fini_naive);\n",
    "            #----------- register the estimated selections  ------------#\n",
    "            selections_B_SL[id_time_steps, n, n_gamma, :] = copy(selections_B_temp_SL)\n",
    "            selections_L_SL[id_time_steps, n, n_gamma, :] = copy(selections_L_temp_SL)\n",
    "            selections_naive_SL[id_time_steps, n, n_gamma, :] = copy(selections_naive_temp_SL)\n",
    "\n",
    "        end\n",
    "    end\n",
    "end\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9271c26c",
   "metadata": {},
   "source": [
    "## Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c3edabc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output selections \n",
    "# This is temp, ouput also, diagonal covariance elements for errobars\n",
    "\n",
    "fout_selections_Bez = open(dir_out * \"selections_multiple_Dt_reg_cases_Bez.dat\", \"w\")\n",
    "fout_selections_Lin = open(dir_out * \"selections_multiple_Dt_reg_cases_Lin.dat\", \"w\");\n",
    "fout_selections_naive = open(dir_out * \"selections_multiple_Dt_reg_cases_naive.dat\", \"w\");\n",
    "\n",
    "fout_errorbar_Bez = open(dir_out * \"error_bar_multiple_Dt_reg_cases_Bez.dat\", \"w\")\n",
    "fout_errorbar_Lin = open(dir_out * \"error_bar_multiple_Dt_reg_cases_Lin.dat\", \"w\")\n",
    "\n",
    "#------- Output estimated selections and errorbars --------#\n",
    "for n_t in 1:n_time_steps\n",
    "for n_reg in 1:n_gamma_max\n",
    "for n in 1:n_ensemble\n",
    "for i in 1:L\n",
    "    println(fout_selections_Lin, n_t, \" \", time_steps[n_t], \" \", n_reg,  \" \", gamma_set[n_reg], \" \", \n",
    "                    n, \" \", i, \" \", \n",
    "                    selections_L[n_t, n, n_reg, i], \" \", selections_L_SL[n_t, n, n_reg, i])\n",
    "    println(fout_selections_Bez, n_t, \" \", time_steps[n_t], \" \", n_reg,  \" \", gamma_set[n_reg], \" \", \n",
    "                    n, \" \", i, \" \", \n",
    "                    selections_B[n_t, n, n_reg, i], \" \", selections_B_SL[n_t, n, n_reg, i])\n",
    "                \n",
    "    println(fout_selections_naive, n_t, \" \", time_steps[n_t], \" \", n_reg,  \" \", gamma_set[n_reg], \" \", \n",
    "                    n, \" \", i, \" \", \n",
    "                    selections_naive[n_t, n, n_reg, i], \" \", selections_naive_SL[n_t, n, n_reg, i])                \n",
    "    \n",
    "    if(n_reg==1)\n",
    "        #----- errobrbar ---- #\n",
    "        println(fout_errorbar_Lin, n_t, \" \", time_steps[n_t], \" \", \n",
    "                            n, \" \", i, \" \",\n",
    "                            errorbar_Lin[n_t, n, i])\n",
    "        println(fout_errorbar_Bez, n_t, \" \", time_steps[n_t], \" \", \n",
    "                            n, \" \", i, \" \", \n",
    "                            errorbar_Bez[n_t, n, i])                \n",
    "    end\n",
    "end\n",
    "end\n",
    "end\n",
    "end\n",
    "close(fout_selections_Lin); close(fout_selections_Bez); close(fout_selections_naive);\n",
    "close(fout_errorbar_Bez); close(fout_errorbar_Lin);\n",
    "\n",
    "#------- Output eivenvalue distributions --------#\n",
    "fout_eigenvalue_Bez = open(dir_out * \"eigenvalues_cov_Bez.dat\", \"w\")\n",
    "fout_eigenvalue_Lin = open(dir_out * \"eigenvalues_cov_Lin.dat\", \"w\");\n",
    "fout_eigenvalue_naive = open(dir_out * \"eigenvalues_cov_naive.dat\", \"w\");\n",
    " \n",
    "for n_t in 1:n_time_steps\n",
    "    for n in 1:n_ensemble\n",
    "        print(fout_eigenvalue_Lin, eigen_min_L[n_t, n], \" \")\n",
    "        print(fout_eigenvalue_Bez, eigen_min_B[n_t, n], \" \")\n",
    "        print(fout_eigenvalue_naive, eigen_min_naive[n_t, n], \" \")\n",
    "    end\n",
    "    println(fout_eigenvalue_Lin, \"\")\n",
    "    println(fout_eigenvalue_Bez, \"\")\n",
    "    println(fout_eigenvalue_naive, \"\")\n",
    "end\n",
    "close(fout_eigenvalue_Lin); close(fout_eigenvalue_Bez); close(fout_eigenvalue_naive);\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e72e0a3d",
   "metadata": {},
   "source": [
    "# Sampling interval, Δt dependncy on covarinace interpolation erorr."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b8db899e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1369.020575 seconds (11.80 G allocations: 4.489 TiB, 32.09% gc time)\n"
     ]
    }
   ],
   "source": [
    "# This is fore creating evolution of covarinaces \n",
    "n_time_steps = size(time_steps,1)\n",
    "covariance_error_L = zeros(6, n_time_steps, n_ensemble) \n",
    "covariance_error_B = zeros(6, n_time_steps, n_ensemble)\n",
    "\n",
    "\n",
    "@time for n in 1:n_ensemble     \n",
    "    #---- computation of the integrated covariance for Δt==1 ------#\n",
    "    #fname_in = \"../data/WF/allele-traject-time-1_id-\" *string(n)* \"_mu-0.001_N-1000_standard.txt\"\n",
    "\n",
    "    fname_in = data_dir * file_key1 *string(n)* file_key2\n",
    "    #test\n",
    "    #fname_in = \"../data_temp/WF/N100/allele-traject_id-\" *string(n)* \"_mu-0.001_N-100_WF_standard.txt\"\n",
    "    data = readdlm(fname_in);\n",
    "    read_upto = count(data[:,1] .<= time_cutoff)\n",
    "    data = copy(data[1:read_upto, :])\n",
    "\n",
    "    time_list = get_time_list(data);\n",
    "    L = size(data,2)-3\n",
    "    N = size(data,1)\n",
    "    unique_time = unique(time_list);\n",
    "    \n",
    "    (C_tot_B_0, drift_B, x1_mean_B, x1_init_B, x1_fini_B, point_set_x1_a_B, point_set_x1_b_B, point_set_x2_a_B, point_set_x2_b_B) = get_Bezier_Ctot_drift_x1mean_binary_simple(L, data, unique_time);\n",
    "    time_interval_threshold = 90\n",
    "    #(C_tot_B_0, drift_B_0, x1_mean_B, x1_init_B, x1_fini_B, point_set_x1_a_B, point_set_x1_b_B, point_set_x2_a_B, point_set_x2_b_B) = get_Bezier_Ctot_drift_x1mean_binary_simple_with_midpoints(L, data, unique_time, time_interval_threshold);\n",
    "    (C_tot_L_0, drift_L_0, x1_mean_L, x1_init_L, x1_fini_L) = get_integrated_Ctot_drift_x1mean_binary_simple(L, data, unique_time);    \n",
    "    \n",
    "    # compute the normalization terms.\n",
    "    norms_L_0 = get_norm_measures(C_tot_L_0)\n",
    "    norms_B_0 = get_norm_measures(C_tot_B_0)\n",
    "    \n",
    "    for id_time_steps in 1:n_time_steps\n",
    "        #----------- obtaining data ---------------#\n",
    "        t_sampling = time_steps[id_time_steps]\n",
    "        #fname_in = \"../data/WF/allele-traject-time-1_id-\" *string(n)* \"_mu-0.001_N-1000_standard.txt\"\n",
    "        fname_in = data_dir * file_key1 *string(n)* file_key2\n",
    "        \n",
    "        #fname_in = \"../data_temp/WF/N100/allele-traject_id-\" *string(n)* \"_mu-0.001_N-100_WF_standard.txt\"\n",
    "        data = readdlm(fname_in);\n",
    "        read_upto = count(data[:,1] .<= time_cutoff)\n",
    "        data = copy(data[1:read_upto, :])\n",
    "        \n",
    "        ## selecting samples those collecting times are each t steps, 0, 30, 60, etc.\n",
    "        selecting_indices = collect(1:size(data,1))[data[:,1] .% t_sampling .== 0]\n",
    "        data = data[selecting_indices,:];\n",
    "                \n",
    "        time_list = get_time_list(data);\n",
    "        L = size(data,2)-3\n",
    "        N = size(data,1)\n",
    "        unique_time = unique(time_list);\n",
    "        \n",
    "        #------------ obtaining covariances and drifts -----------#\n",
    "        (C_tot_B, drift_B, x1_mean_B, x1_init_B, x1_fini_B, point_set_x1_a_B, point_set_x1_b_B, point_set_x2_a_B, point_set_x2_b_B) = get_Bezier_Ctot_drift_x1mean_binary_simple(L, data, unique_time);\n",
    "        #(C_tot_B, drift_B, x1_mean_B, x1_init_B, x1_fini_B, point_set_x1_a_B, point_set_x1_b_B, point_set_x2_a_B, point_set_x2_b_B) = get_Bezier_Ctot_drift_x1mean_binary_simple_with_midpoints(L, data, unique_time, time_interval_threshold);\n",
    "        (C_tot_L, drift_L, x1_mean_L, x1_init_L, x1_fini_L) = get_integrated_Ctot_drift_x1mean_binary_simple(L, data, unique_time);\n",
    "        \n",
    "        norms_L = get_norm_measures(C_tot_L - C_tot_L_0)\n",
    "        norms_B = get_norm_measures(C_tot_B - C_tot_B_0)\n",
    "        \n",
    "        covariance_error_L[:, id_time_steps, n] = norms_L ./ norms_L_0 \n",
    "        covariance_error_B[:, id_time_steps, n] = norms_B ./ norms_B_0 \n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e04060af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get mean and standard deviation values.\n",
    "covariance_error_L_mean = zeros(6, n_time_steps)\n",
    "covariance_error_L_std = zeros(6, n_time_steps)\n",
    "covariance_error_B_mean = zeros(6, n_time_steps)\n",
    "covariance_error_B_std = zeros(6, n_time_steps)\n",
    "for k in 1:6\n",
    "    for n_t in 1:n_time_steps\n",
    "        covariance_error_L_mean[k, n_t] = summarystats(covariance_error_L[k, n_t, :]).mean\n",
    "        covariance_error_L_std[k, n_t] = std(covariance_error_L[k, n_t, :]) / sqrt(n_ensemble)        \n",
    "        covariance_error_B_mean[k, n_t] = summarystats(covariance_error_B[k, n_t, :]).mean\n",
    "        covariance_error_B_std[k, n_t] = std(covariance_error_B[k, n_t, :]) / sqrt(n_ensemble)\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9922730c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output covariance interpolation errors\n",
    "fout_cov_error_Lin_mean = open(dir_out * \"covariance_error_Lin_mean.dat\", \"w\");\n",
    "fout_cov_error_Lin_std = open(dir_out * \"covariance_error_Lin_std.dat\", \"w\");\n",
    "fout_cov_error_Bez_mean = open(dir_out * \"covariance_error_Bez_mean.dat\", \"w\")\n",
    "fout_cov_error_Bez_std = open(dir_out * \"covariance_error_Bez_std.dat\", \"w\");\n",
    "\n",
    "i_max, j_max = size(covariance_error_L_mean)\n",
    "for i in 1:i_max\n",
    "    for j in 1:j_max\n",
    "        print(fout_cov_error_Lin_mean, covariance_error_L_mean[i,j], \" \")\n",
    "        print(fout_cov_error_Lin_std, covariance_error_L_std[i,j], \" \")\n",
    "        print(fout_cov_error_Bez_mean, covariance_error_B_mean[i,j], \" \")\n",
    "        print(fout_cov_error_Bez_std, covariance_error_B_std[i,j], \" \")\n",
    "    end\n",
    "    println(fout_cov_error_Lin_mean)\n",
    "    println(fout_cov_error_Lin_std)\n",
    "    println(fout_cov_error_Bez_mean)\n",
    "    println(fout_cov_error_Bez_std)\n",
    "    \n",
    "end\n",
    "close(fout_cov_error_Lin_mean)\n",
    "close(fout_cov_error_Lin_std)\n",
    "close(fout_cov_error_Bez_mean)\n",
    "close(fout_cov_error_Bez_std);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f1639c0",
   "metadata": {},
   "source": [
    "# Autocorrelations for the pairwise frequenceis for the Wright-Fisher model (this computation takes long time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e24ea67c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11223.983517 seconds (171.77 G allocations: 5.433 TiB, 25.58% gc time)\n",
      "9348.296850 seconds (143.72 G allocations: 4.550 TiB, 25.63% gc time)\n"
     ]
    }
   ],
   "source": [
    "##### The following computation takes around 3-4 hours ####\n",
    "T=300\n",
    "N_ensemble=50\n",
    "##### compute the equilibrium states. ############### \n",
    "t_start = 1\n",
    "\n",
    "C_av_tot = zeros(T, L,L); \n",
    "C_t0_tot = zeros(N_ensemble, L,L); \n",
    "scale_N = 1.0/N_ensemble\n",
    "C_var_tot = zeros(T, L, L)\n",
    "\n",
    "@time for t in t_start:T\n",
    "\tC_temp = zeros(N_ensemble, L, L)\t\n",
    "\tfor id_ensemble in 1:N_ensemble\n",
    "        fname_in = data_dir * file_key1 * string(id_ensemble) * file_key2\n",
    "\t    data = readdlm(fname_in)\n",
    "\t    \n",
    "\t    # keep the initial single site freq. and covariance.     \n",
    "\t    if(t==t_start) \n",
    "\t        (n_t0, sample_t0) = get_sample_at_t(data, t)\n",
    "\t        (x1_t0, x2_t0, C_t0) = get_x1_x2_C(L, n_t0, sample_t0);\n",
    "\t        C_t0_tot[id_ensemble, :, :] = C_t0\n",
    "\t    end       \n",
    "\t    #--- computation of the averaged single site freq. and covariance.  \n",
    "\t    time_list = get_time_list(data)\n",
    "\t    (n_t, sample_t) = get_sample_at_t(data, t)\n",
    "\t    (x1_t, x2_t, C_t) = get_x1_x2_C(L, n_t, sample_t);\n",
    "\t    C_av_tot[t,:,:] += C_t\n",
    "        C_temp[id_ensemble, :, :] = copy(C_t)\n",
    "\tend\n",
    "    C_av_tot[t,:,:] *= scale_N \n",
    "\n",
    "\tfor i in 1:L\n",
    "\t\tfor j in i:L\n",
    "            C_var_tot[t, i, j] = std(C_temp[:, i, j])\t\n",
    "            C_var_tot[t, j, i] = C_var_tot[t, i, j]\n",
    "\t\tend\n",
    "\tend\n",
    "end\n",
    "#########################################################\n",
    "\n",
    "#-------------------------------------------------------#\n",
    "##### The following computation takes around 3 hours ####\n",
    "############ compute the AutoC_t50orrelations   #########\n",
    "t_start = 50\n",
    "AutoC_t50 = zeros(2, T)\n",
    "t0 = t_start \n",
    "@time for t in t_start:T\n",
    "    AC_C = zeros(L,L); \n",
    "\tAC_x2 = zeros(L,L); \n",
    "\tAC_x1 = zeros(L)\n",
    "\tfor id_ensemble in 1:N_ensemble\n",
    "        fname_in = data_dir * file_key1 * string(id_ensemble) * file_key2\n",
    "\t    data = readdlm(fname_in)\n",
    "\t    time_list = get_time_list(data)\n",
    "\t    (n_t, sample_t) = get_sample_at_t(data, t)\n",
    "\t    (x1_t, x2_t, C_t) = get_x1_x2_C(L, n_t, sample_t);\t\n",
    "\t    C_t0 = C_t0_tot[id_ensemble, :, :] \n",
    "\t    \n",
    "        AC_C += C_t .* C_t0 \n",
    "\tend\n",
    "    AC_C  = copy(AC_C)  * scale_N - C_av_tot[t,:,:] .* C_av_tot[t0,:,:]\n",
    "\t# normalized by the \n",
    "\t#AC_C  = copy(AC_C)  ./ ( C_var_tot[t0, :,:]  .* C_var_tot[t0, :,:] .+ 1e-3 ) \n",
    "\n",
    "\tAC_diag_C = AC_C[diagind(AC_C)]\t\n",
    "\tAC_offdiag_C = copy(AC_C)\n",
    " \tAC_offdiag_C[diagind(AC_offdiag_C)] .= 0\t\n",
    "\n",
    "    AutoC_t50[1, t] = sum(AC_diag_C) / L \n",
    "\tAutoC_t50[2, t] = sum(AC_offdiag_C)/ ( L*(L-1) )         \t\t\n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "66120811",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output autocorrelation files. \n",
    "fout_acf = open(dir_out * \"autocorrelation_WF.dat\", \"w\");\n",
    "for i in 51:size(AutoC_t50,2)\n",
    "    for j in 1:(size(AutoC_t50,1)-1)\n",
    "        print(fout_acf, AutoC_t50[j,i], \" \")\n",
    "    end\n",
    "    println(fout_acf, AutoC_t50[end,i] )\n",
    "end\n",
    "close(fout_acf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9a5058b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2f15568",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.1.1",
   "language": "julia",
   "name": "julia-1.1"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.1.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
